import gradio as gr
import nemo.collections.asr as nemo_asr
import torch
import os
import sys
import time
from pathlib import Path

# Global model cache to avoid reloading
models_cache = {}

# Video file extensions supported (librosa + FFmpeg backend handles audio extraction)
VIDEO_EXTENSIONS = {'.mp4', '.avi', '.mkv', '.mov', '.webm', '.flv', '.m4v'}

# Audio file extensions
AUDIO_EXTENSIONS = {'.wav', '.mp3', '.flac', '.m4a', '.ogg', '.aac', '.wma'}

# Separator string for output formatting
SEPARATOR = '=' * 60

# Model configurations
MODEL_CONFIGS = {
    "parakeet": {
        "local_path": "local_models/parakeet.nemo",
        "max_batch_size": 32,  # Increased from 16
        "display_name": "Parakeet-TDT-0.6B v2"
    },
    "canary": {
        "local_path": "local_models/canary.nemo", 
        "max_batch_size": 16,  # Increased from default
        "display_name": "Canary-Qwen-2.5B"
    }
}

def get_script_dir():
    """Get the directory where the script is located"""
    return Path(__file__).parent.absolute()

def validate_local_models():
    """
    Validate that local .nemo model files exist before launching UI.
    
    Returns:
        bool: True if all models exist, False otherwise
    """
    script_dir = get_script_dir()
    local_models_dir = script_dir / "local_models"
    
    missing_files = []
    
    # Check if local_models directory exists
    if not local_models_dir.exists():
        print("\n" + "="*80)
        print("‚ùå LOCAL MODELS NOT FOUND!")
        print("="*80)
        print(f"\nThe 'local_models/' directory does not exist.")
        print(f"Expected location: {local_models_dir}")
        print("\nRequired files:")
        print(f"  ‚Ä¢ {local_models_dir / 'parakeet.nemo'}")
        print(f"  ‚Ä¢ {local_models_dir / 'canary.nemo'}")
        print("\nThese files must be created once using the setup script.")
        print("Please run: python setup_local_models.py")
        print("\nSee docs/manual/user-model-setup-guide.md for instructions.")
        print("="*80 + "\n")
        return False
    
    # Check for each .nemo file
    for model_name, config in MODEL_CONFIGS.items():
        model_path = script_dir / config["local_path"]
        if not model_path.exists():
            missing_files.append((model_name, model_path))
    
    if missing_files:
        print("\n" + "="*80)
        print("‚ùå LOCAL MODELS NOT FOUND!")
        print("="*80)
        print("\nMissing model files:")
        for model_name, path in missing_files:
            print(f"  ‚Ä¢ {path} ({MODEL_CONFIGS[model_name]['display_name']})")
        print("\nThese files must be created once using the setup script.")
        print("Please run: python setup_local_models.py")
        print("\nSee docs/manual/user-model-setup-guide.md for instructions.")
        print("="*80 + "\n")
        return False
    
    return True

def get_dynamic_batch_size(duration, model_key):
    """Calculate optimal batch size based on audio duration and model"""
    max_batch = MODEL_CONFIGS[model_key]["max_batch_size"]
    
    if duration < 60:  # Under 1 minute
        return min(8, max_batch)
    elif duration < 300:  # Under 5 minutes
        return min(16, max_batch)
    elif duration < 900:  # Under 15 minutes
        return min(24, max_batch)
    else:  # Longer audio
        return max_batch

def setup_gpu_optimizations():
    """Enable GPU optimizations for better performance"""
    if torch.cuda.is_available():
        # Enable TF32 for matrix multiplication (Ampere+ GPUs)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        # Enable cuDNN benchmark for faster convolutions
        torch.backends.cudnn.benchmark = True
        print("‚úÖ GPU optimizations enabled (TF32, cuDNN benchmark)")

def load_model(model_name, show_progress=False):
    """Load model from local .nemo file if not already in memory cache.
    
    Args:
        model_name: "parakeet" or "canary"
        show_progress: Whether to show loading progress (for startup)
    
    Returns:
        The loaded ASR model
        
    Raises:
        FileNotFoundError: If the .nemo file doesn't exist
    """
    if model_name not in models_cache:
        config = MODEL_CONFIGS[model_name]
        script_dir = get_script_dir()
        model_path = script_dir / config["local_path"]
        
        # Check if .nemo file exists
        if not model_path.exists():
            error_msg = (
                f"\n{'='*80}\n"
                f"‚ùå MODEL FILE NOT FOUND!\n"
                f"{'='*80}\n\n"
                f"Missing: {model_path}\n"
                f"Model: {config['display_name']}\n\n"
                f"The .nemo file must be created once using the setup script.\n"
                f"Please run: python setup_local_models.py\n\n"
                f"See docs/manual/user-model-setup-guide.md for instructions.\n"
                f"{'='*80}\n"
            )
            raise FileNotFoundError(error_msg)
        
        print(f"üì¶ Loading {config['display_name']} from local file...")
        
        start_time = time.time()
        try:
            models_cache[model_name] = nemo_asr.models.ASRModel.restore_from(
                str(model_path)
            )
        except FileNotFoundError as e:
            # Re-raise with more helpful message
            error_msg = (
                f"\n{'='*80}\n"
                f"‚ùå FAILED TO LOAD MODEL!\n"
                f"{'='*80}\n\n"
                f"File path: {model_path}\n"
                f"Model: {config['display_name']}\n"
                f"Original error: {str(e)}\n\n"
                f"The .nemo file may be corrupted or incomplete.\n"
                f"Please recreate it using: python setup_local_models.py\n\n"
                f"See docs/manual/user-model-setup-guide.md for instructions.\n"
                f"{'='*80}\n"
            )
            raise FileNotFoundError(error_msg)
        
        load_time = time.time() - start_time
        print(f"‚úì {config['display_name']} loaded in {load_time:.1f}s")
    return models_cache[model_name]

def transcribe_audio(audio_files, model_choice, save_to_file, include_timestamps):
    """
    Main transcription function with batch processing, video support, and GPU optimization
    
    Args:
        audio_files: Path to uploaded audio/video file OR list of paths for batch processing
        model_choice: "Parakeet-TDT-0.6B v2 (Fast)" or "Canary-Qwen-2.5B (Accurate)"
        save_to_file: Boolean - whether to save .txt file
        include_timestamps: Boolean - whether to generate timestamps
    
    Returns:
        status_message: HTML formatted status
        transcription_text: The actual transcription(s)
        download_file: Path to saved file (if save_to_file=True)
    """
    
    if audio_files is None:
        return "‚ö†Ô∏è Please upload an audio or video file first", "", None
    
    try:
        import librosa
        
        # Handle both single file and multiple files
        # Gradio gr.File with file_count="multiple" returns list of file objects
        # Each file object has a .name attribute with the path
        if audio_files is None or (isinstance(audio_files, list) and len(audio_files) == 0):
            return "‚ö†Ô∏è Please upload an audio or video file first", "", None
        
        # Convert file objects to paths
        if isinstance(audio_files, str):
            file_list = [audio_files]
        elif isinstance(audio_files, list):
            # Handle list of file objects or strings
            file_list = []
            for f in audio_files:
                if hasattr(f, 'name'):
                    file_list.append(f.name)
                else:
                    file_list.append(str(f))
        elif hasattr(audio_files, 'name'):
            file_list = [audio_files.name]
        else:
            file_list = [str(audio_files)]
        
        is_batch = len(file_list) > 1
        
        # Determine which model to use
        model_key = "parakeet" if "Parakeet" in model_choice else "canary"
        
        # Start timing
        start_time = time.time()
        
        # Load model (uses cache if already loaded)
        model = load_model(model_key)
        load_time = time.time() - start_time
        
        # Process files and detect video files
        processed_files = []
        file_info = []
        total_duration = 0
        video_count = 0
        
        for file_path in file_list:
            file_ext = os.path.splitext(file_path)[1].lower()
            is_video = file_ext in VIDEO_EXTENSIONS
            
            if is_video:
                video_count += 1
                print(f"üé¨ Extracting audio from video: {os.path.basename(file_path)}")
            
            # librosa.get_duration handles both audio and video files (via FFmpeg)
            try:
                duration = librosa.get_duration(path=file_path)
            except Exception as e:
                # Handle case where video has no audio track
                if is_video:
                    return f"‚ùå Video file '{os.path.basename(file_path)}' appears to have no audio track or cannot be processed.\n\nError: {str(e)}", "", None
                raise
            
            total_duration += duration
            processed_files.append(file_path)
            file_info.append({
                "path": file_path,
                "name": os.path.basename(file_path),
                "duration": duration,
                "is_video": is_video
            })
        
        # Calculate optimal batch size based on total duration
        batch_size = get_dynamic_batch_size(total_duration, model_key)
        
        # Prepare status update for video processing
        video_status = ""
        if video_count > 0:
            video_status = f"üé¨ Extracted audio from {video_count} video file(s)\n"
        
        # Transcribe with mixed precision (FP16) for GPU acceleration
        inference_start = time.time()
        
        if torch.cuda.is_available():
            # Use mixed precision (FP16) for faster inference on CUDA
            with torch.autocast(device_type='cuda', dtype=torch.float16):
                result = model.transcribe(
                    processed_files, 
                    batch_size=batch_size,
                    timestamps=include_timestamps
                )
        else:
            # CPU fallback - no autocast
            result = model.transcribe(
                processed_files, 
                batch_size=batch_size,
                timestamps=include_timestamps
            )
        
        inference_time = time.time() - inference_start
        total_time = time.time() - start_time
        
        # Get GPU stats
        if torch.cuda.is_available():
            vram_used = torch.cuda.memory_allocated() / 1024**3
            gpu_name = torch.cuda.get_device_name(0)
        else:
            vram_used = 0
            gpu_name = "CPU"
        
        # Calculate real-time factor
        rtfx = total_duration / inference_time if inference_time > 0 else 0
        
        # Build output based on single vs batch processing
        if is_batch:
            # Batch processing output
            all_transcriptions = []
            per_file_stats = []
            
            for i, (res, info) in enumerate(zip(result, file_info)):
                transcription = res.text
                all_transcriptions.append(transcription)
                
                file_duration = info["duration"]
                file_mins = int(file_duration // 60)
                file_secs = int(file_duration % 60)
                file_type = "üé¨ Video" if info["is_video"] else "üéµ Audio"
                
                per_file_stats.append(
                    f"**{i+1}. {info['name']}** ({file_type})\n"
                    f"   - Duration: {file_mins}m {file_secs}s\n"
                    f"   - Words: {len(transcription.split())}"
                )
            
            total_mins = int(total_duration // 60)
            total_secs = int(total_duration % 60)
            total_words = sum(len(t.split()) for t in all_transcriptions)
            
            status = f"""
### ‚úÖ Batch Transcription Complete!

{video_status}**üìä Overall Statistics:**
- **Files Processed**: {len(file_list)}
- **Model**: {model_choice}
- **GPU**: {gpu_name}
- **Total Audio Duration**: {total_mins}m {total_secs}s
- **Processing Time**: {total_time:.2f} seconds
- **Inference Time**: {inference_time:.2f} seconds
- **Batch Size Used**: {batch_size}
- **Real-Time Factor**: {rtfx:.1f}√ó (processed {rtfx:.1f}√ó faster than real-time)
- **VRAM Used**: {vram_used:.2f} GB
- **Total Words**: {total_words}

**üìÅ Per-File Statistics:**
{chr(10).join(per_file_stats)}

---
"""
            # Combine transcriptions with file headers
            combined_transcription = ""
            for i, (info, trans) in enumerate(zip(file_info, all_transcriptions)):
                combined_transcription += f"\n{SEPARATOR}\n"
                combined_transcription += f"FILE {i+1}: {info['name']}\n"
                combined_transcription += f"{SEPARATOR}\n\n"
                combined_transcription += trans + "\n"
            
            transcription_output = combined_transcription
            
        else:
            # Single file processing (original behavior)
            transcription = result[0].text
            info = file_info[0]
            
            minutes = int(info["duration"] // 60)
            seconds = int(info["duration"] % 60)
            
            # Format timestamps if requested
            timestamp_text = ""
            if include_timestamps and hasattr(result[0], 'words') and result[0].words:
                timestamp_text = "\n\n### Word-Level Timestamps (first 50 words):\n\n"
                for i, word in enumerate(result[0].words[:50]):
                    timestamp_text += f"`{word.start:.2f}s - {word.end:.2f}s` ‚Üí **{word.text}**\n\n"
                if len(result[0].words) > 50:
                    timestamp_text += f"\n*...and {len(result[0].words) - 50} more words*"
            
            file_type_msg = "üé¨ Video file detected - audio extracted automatically\n" if info["is_video"] else ""
            
            status = f"""
### ‚úÖ Transcription Complete!

{file_type_msg}**üìä Statistics:**
- **Model**: {model_choice}
- **GPU**: {gpu_name}
- **Audio Duration**: {minutes}m {seconds}s
- **Processing Time**: {total_time:.2f} seconds
- **Inference Time**: {inference_time:.2f} seconds
- **Model Load Time**: {load_time:.2f} seconds
- **Batch Size Used**: {batch_size}
- **Real-Time Factor**: {rtfx:.1f}√ó (processed {rtfx:.1f}√ó faster than real-time)
- **VRAM Used**: {vram_used:.2f} GB
- **Transcription Length**: {len(transcription)} characters ({len(transcription.split())} words)

---
"""
            transcription_output = transcription + timestamp_text
        
        # Save to file if requested
        output_file = None
        if save_to_file:
            if is_batch:
                output_file = f"batch_transcription_{len(file_list)}_files.txt"
            else:
                base_name = os.path.splitext(os.path.basename(file_list[0]))[0]
                output_file = f"{base_name}_transcription.txt"
            
            with open(output_file, "w", encoding="utf-8") as f:
                if is_batch:
                    f.write(f"Batch Transcription - {len(file_list)} files\n")
                    f.write(f"Model: {model_choice}\n")
                    f.write(f"Total Duration: {int(total_duration // 60)}m {int(total_duration % 60)}s\n")
                    f.write(f"Processing Time: {total_time:.2f}s\n")
                    f.write(f"\n{SEPARATOR}\n")
                    
                    for i, (info, trans) in enumerate(zip(file_info, all_transcriptions)):
                        f.write(f"\nFILE {i+1}: {info['name']}\n")
                        f.write(f"Duration: {int(info['duration'] // 60)}m {int(info['duration'] % 60)}s\n")
                        f.write(f"{SEPARATOR}\n\n")
                        f.write(trans)
                        f.write("\n")
                else:
                    info = file_info[0]
                    f.write(f"Audio File: {info['name']}\n")
                    f.write(f"Model: {model_choice}\n")
                    f.write(f"Duration: {int(info['duration'] // 60)}m {int(info['duration'] % 60)}s\n")
                    f.write(f"Processing Time: {total_time:.2f}s\n")
                    f.write(f"\n{SEPARATOR}\n")
                    f.write(f"TRANSCRIPTION\n")
                    f.write(f"{SEPARATOR}\n\n")
                    f.write(transcription)
                    
                    if include_timestamps and hasattr(result[0], 'words') and result[0].words:
                        f.write(f"\n\n{SEPARATOR}\n")
                        f.write(f"WORD-LEVEL TIMESTAMPS\n")
                        f.write(f"{SEPARATOR}\n\n")
                        for word in result[0].words:
                            f.write(f"{word.start:.2f}s - {word.end:.2f}s: {word.text}\n")
            
            status += f"\nüíæ **Saved to**: `{output_file}`"
        
        return status, transcription_output, output_file
        
    except Exception as e:
        # Get actual VRAM info for error message
        if torch.cuda.is_available():
            vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            vram_info = f"you have {vram_total:.0f}GB"
        else:
            vram_info = "no GPU detected"
        
        error_msg = f"""
### ‚ùå Error During Transcription

**Error Type**: {type(e).__name__}

**Error Message**: {str(e)}

**Troubleshooting:**
1. Make sure the audio/video file is valid
2. Check that you have enough VRAM ({vram_info})
3. Try a shorter audio file first
4. Restart the interface if models are stuck
5. For video files, ensure FFmpeg is installed

If the error persists, please check the console output for more details.
"""
        return error_msg, "", None

def get_system_info():
    """Display system information"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        vram_free = (torch.cuda.get_device_properties(0).total_memory - 
                     torch.cuda.memory_allocated()) / 1024**3
        
        info = f"""
### üñ•Ô∏è System Information

**GPU**: {gpu_name}
**Total VRAM**: {vram_total:.1f} GB
**Available VRAM**: {vram_free:.1f} GB
**CUDA Version**: {torch.version.cuda}
**PyTorch Version**: {torch.__version__}
**NeMo Available**: ‚úÖ Yes

**Status**: ‚úÖ Ready for transcription
"""
    else:
        info = """
### ‚ö†Ô∏è No GPU Detected

CUDA is not available. Please check:
1. NVIDIA GPU drivers are installed
2. CUDA Toolkit is installed
3. PyTorch was installed with CUDA support
"""
    return info

def get_privacy_performance_info():
    """Generate dynamic privacy & performance information"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        gpu_info = f"**GPU**: {gpu_name} ({vram_total:.0f}GB VRAM)"
    else:
        gpu_info = "**GPU**: CPU Mode (No CUDA GPU detected)"
    
    return f"""
### Privacy:
- ‚úÖ 100% local & offline processing
- ‚úÖ No internet connection required
- ‚úÖ Your audio never leaves your computer
- ‚úÖ Models stored locally as .nemo files

### Performance Optimizations:
- {gpu_info}
- **Speed**: 40-100√ó faster than real-time
- **Example**: 1 hour audio ‚Üí 30-60 seconds processing
- **Mixed Precision**: FP16 inference for 1.5-2.5√ó speedup
- **TF32 Tensor Cores**: Enabled for matrix operations
- **Dynamic Batch Sizing**: Optimized based on audio duration
- **Local Loading**: Fast model loading from local .nemo files
"""

# Create the Gradio interface
with gr.Blocks(title="üéôÔ∏è Local ASR Transcription") as app:
    
    # Header
    gr.Markdown("""
    # üéôÔ∏è NVIDIA NeMo Local Audio Transcription
    ### 100% Offline - Powered by Local Parakeet-TDT & Canary-Qwen Models
    
    Transform your audio and video files into accurate text transcriptions using state-of-the-art AI models stored locally on your system. No internet required.
    """)
    
    # System info at the top
    with gr.Accordion("üìä System Information", open=False):
        system_info = gr.Markdown(get_system_info())
        refresh_btn = gr.Button("üîÑ Refresh System Info", size="sm")
        refresh_btn.click(fn=get_system_info, outputs=system_info)
    
    gr.Markdown("---")
    
    # Main interface
    with gr.Row():
        # Left column - Input
        with gr.Column(scale=1):
            gr.Markdown("### üìÇ Upload Audio/Video Files")
            
            # Use gr.File for multiple file uploads (audio and video)
            audio_input = gr.File(
                file_count="multiple",
                file_types=[
                    ".wav", ".mp3", ".flac", ".m4a", ".ogg", ".aac", ".wma",  # Audio
                    ".mp4", ".avi", ".mkv", ".mov", ".webm", ".flv", ".m4v"   # Video
                ],
                label="Audio/Video Files (select multiple)"
            )
            
            gr.Markdown("""
**Supported audio formats**: WAV, MP3, FLAC, M4A, OGG, AAC, WMA

**Supported video formats**: MP4, AVI, MKV, MOV, WEBM, FLV, M4V
*(Audio will be automatically extracted from video files)*

üí° **Tip**: Select multiple files for batch processing!
""")
            
            gr.Markdown("### ‚öôÔ∏è Settings")
            
            model_selector = gr.Radio(
                choices=[
                    "Parakeet-TDT-0.6B v2 (Fast)",
                    "Canary-Qwen-2.5B (Accurate)"
                ],
                value="Parakeet-TDT-0.6B v2 (Fast)",
                label="Model Selection",
                info="Parakeet: Faster processing | Canary: Higher accuracy"
            )
            
            save_checkbox = gr.Checkbox(
                label="üíæ Save transcription to .txt file",
                value=True,
                info="Creates a text file in the same directory"
            )
            
            timestamp_checkbox = gr.Checkbox(
                label="‚è±Ô∏è Include word-level timestamps",
                value=False,
                info="Shows when each word was spoken"
            )
            
            transcribe_btn = gr.Button(
                "üöÄ Start Transcription",
                variant="primary",
                size="lg"
            )
            
            gr.Markdown("""
            ---
            ### üìñ Model Information
            
            **Parakeet-TDT-0.6B v2:**
            - Speed: ~40-60√ó real-time
            - Accuracy: 93.32% (6.68% WER)
            - Best for: Quick transcriptions, bulk processing
            - Loads from: `local_models/parakeet.nemo`
            
            **Canary-Qwen-2.5B:**
            - Speed: ~50-100√ó real-time
            - Accuracy: 94.37% (5.63% WER)  
            - Best for: Critical accuracy, technical content
            - Loads from: `local_models/canary.nemo`
            """)
        
        # Right column - Output
        with gr.Column(scale=2):
            gr.Markdown("### üìù Transcription Results")
            
            status_output = gr.Markdown(
                "Upload an audio file and click 'Start Transcription' to begin..."
            )
            
            # FIXED: Replaced show_copy_button with buttons parameter
            transcription_output = gr.Textbox(
                label="Transcription Text",
                lines=20,
                placeholder="Your transcription will appear here...",
                buttons=["copy"],  # NEW: Use buttons parameter instead
                show_label=True
            )
            
            file_output = gr.File(
                label="üì• Download Transcription File",
                visible=True
            )
    
    # Bottom section
    gr.Markdown("---")
    
    with gr.Accordion("‚ùì How to Use", open=False):
        gr.Markdown("""
        ### Quick Start:
        
        1. **Upload files** (audio or video - select multiple for batch processing)
        2. **Select model** (Parakeet for speed, Canary for accuracy)
        3. **Click "Start Transcription"**
        4. **Copy or download** your transcription
        
        ### Features:
        - üéµ **Audio Support**: WAV, MP3, FLAC, M4A, OGG, AAC, WMA
        - üé¨ **Video Support**: MP4, AVI, MKV, MOV, WEBM, FLV, M4V
        - üì¶ **Batch Processing**: Upload multiple files at once
        - ‚ö° **GPU Optimized**: Uses FP16 mixed precision for faster processing
        - üîí **100% Offline**: Models load from local files, no internet needed
        
        ### Tips:
        - Models load from local `.nemo` files in the `local_models/` folder
        - First transcription loads model into memory (~2-3 seconds)
        - Subsequent transcriptions reuse cached model (instant)
        - Processing time: ~30-60 seconds per hour of audio
        - Video files: Audio is automatically extracted
        
        ### Setup:
        - Models must be set up once using `setup_local_models.py`
        - See `docs/manual/user-model-setup-guide.md` for instructions
        """)
    
    with gr.Accordion("üîí Privacy & Performance", open=False):
        gr.Markdown(get_privacy_performance_info())
    
    # Connect button
    transcribe_btn.click(
        fn=transcribe_audio,
        inputs=[audio_input, model_selector, save_checkbox, timestamp_checkbox],
        outputs=[status_output, transcription_output, file_output]
    )

# Launch
if __name__ == "__main__":
    print("\n" + "="*80)
    print("üöÄ Starting NVIDIA NeMo Transcription Interface")
    print("="*80)
    
    if torch.cuda.is_available():
        print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        print(f"‚úÖ CUDA: {torch.version.cuda}")
        
        # Enable GPU optimizations (TF32, cuDNN)
        setup_gpu_optimizations()
    else:
        print("‚ö†Ô∏è  WARNING: No CUDA GPU detected!")
    
    print("="*80)
    
    # Validate local models exist before launching UI
    print("\nüì¶ Checking local model files...")
    script_dir = get_script_dir()
    for model_name, config in MODEL_CONFIGS.items():
        model_path = script_dir / config["local_path"]
        status = "‚úÖ Found" if model_path.exists() else "‚ùå Missing"
        print(f"   {config['display_name']}: {status}")
        if model_path.exists():
            size_mb = model_path.stat().st_size / (1024 * 1024)
            print(f"      Path: {model_path}")
            print(f"      Size: {size_mb:.1f} MB")
    
    if not validate_local_models():
        sys.exit(1)
    
    print("\n" + "="*80)
    print("\nüåê Opening in browser at: http://127.0.0.1:7860")
    print("üí° Keep this terminal open while using the interface")
    print("üõë Press Ctrl+C to stop\n")
    
    app.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        inbrowser=True,
        show_error=True
    )
